{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yuytuIllsv1"
      },
      "source": [
        "# Assignment 3, Part 2: T5 SQuAD Model\n",
        "\n",
        "Welcome to the part 2 of testing the models for this week's assignment. This time we will perform decoding using the T5 SQuAD model. In this notebook we'll perform Question Answering by providing a \"Question\", its \"Context\" and see how well we get the \"Target\" answer.\n",
        "\n",
        "## IMPORTANT\n",
        "\n",
        "- As you cannot save the changes you make to this colab, you have to make a copy of this notebook in your own drive and run that. You can do so by going to `File -> Save a copy in Drive`. Close this colab and open the copy which you have made in your own drive.\n",
        "\n",
        "- Go to this [google drive folder](https://drive.google.com/drive/folders/1v3-aEfvs8ZMNRvEl36O7Hd5_pzszqTSz?usp=sharing) named `NLP_C4_W3_Colabs`. In the folder, next to its name use the drop down menu to select `\"Organise -> Add shortcut\"`, when the pop up opens, select `All Locations` then select `My Drive` and then press `Add`. This should add a shortcut to the folder `NLP_C4_W3_Colabs` within your own google drive. Please make sure this happens, as you'll be reading the data for this notebook from this folder.\n",
        "\n",
        "- Make sure your runtime is GPU (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db6LQW5cMSgx"
      },
      "source": [
        "**Note: Restarting the runtime maybe required**.\n",
        "\n",
        "Colab will tell you if the restarting is necessary -- you can do this from the:\n",
        "\n",
        "Runtime > Restart Runtime\n",
        "\n",
        "option in the dropdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTeAcYQo_4OW"
      },
      "source": [
        "## Outline\n",
        "\n",
        "- [Part 0: Downloading and loading dependencies](#0)\n",
        "- [Part 1: Mounting your drive for data accessibility](#1)\n",
        "- [Part 2: Getting things ready](#2)\n",
        "- [Part 3: Fine-tuning on SQuAD](#3)\n",
        "    - [3.1 Loading in the data and preprocessing](#3.1)\n",
        "    - [3.2 Decoding from a fine-tuned model](#3.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxogfC1M158"
      },
      "source": [
        "<a name='0'></a>\n",
        "# Part 0: Downloading and loading dependencies\n",
        "\n",
        "Uncomment the code cell below and run it to download some dependencies that you will need. You need to download them once every time you open the colab. You can ignore the `kfac` error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BNZzCg0xv3R",
        "outputId": "61baf0e0-855f-4dbf-f9e6-35adf4506dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip -q install trax\n",
        "!pip -q install t5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m637.9/637.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.5/164.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.4/353.4 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.5/568.5 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDhi6qLQMHzs"
      },
      "source": [
        "import string\n",
        "import t5\n",
        "import numpy as np\n",
        "import trax\n",
        "from trax.supervised import decoding\n",
        "import textwrap\n",
        "# Will come handy later.\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwr7LoXwQUW5"
      },
      "source": [
        "<a name='1'></a>\n",
        "# Part 1: Mounting your drive for data accessibility\n",
        "\n",
        "Run the code cell below and follow the instructions to mount your drive. The data is the same as used in the coursera version of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ZF7KiXzQEg",
        "outputId": "97a2fcf2-4b5c-4e14-c94b-e0542a09dbe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
            "  self._read_thread.setDaemon(True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc2hyEgkUppw"
      },
      "source": [
        "path = \"/content/drive/My Drive/NLP_C4_W3_Colabs\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuTyft5EBQK6"
      },
      "source": [
        "<a name='2'></a>\n",
        "# Part 2: Getting things ready\n",
        "\n",
        "Run the code cell below to ready some functions which will later help us in decoding. The code and the functions are the same as the ones you previsouly ran on the coursera version of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ8s_xZ1QtkI",
        "outputId": "e6c64831-9815-4516-9731-23da8923f98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\n",
        "\n",
        "def detokenize(np_array):\n",
        "  return trax.data.detokenize(\n",
        "      np_array,\n",
        "      vocab_type = 'sentencepiece',\n",
        "      vocab_file = 'sentencepiece.model',\n",
        "      vocab_dir = path + \"/models/\")\n",
        "\n",
        "def tokenize(s):\n",
        "  # The trax.data.tokenize function operates on streams,\n",
        "  # that's why we have to create 1-element stream with iter\n",
        "  # and later retrieve the result with next.\n",
        "  return next(trax.data.tokenize(\n",
        "      iter([s]),\n",
        "      vocab_type = 'sentencepiece',\n",
        "      vocab_file = 'sentencepiece.model',\n",
        "      vocab_dir = path + \"/models/\"))\n",
        "\n",
        "vocab_size = trax.data.vocab_size(\n",
        "    vocab_type = 'sentencepiece',\n",
        "    vocab_file = 'sentencepiece.model',\n",
        "    vocab_dir = path + \"/models/\")\n",
        "\n",
        "def get_sentinels(vocab_size):\n",
        "    sentinels = {}\n",
        "\n",
        "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
        "\n",
        "        decoded_text = detokenize([vocab_size - i])\n",
        "\n",
        "        # Sentinels, ex: <Z> - <a>\n",
        "        sentinels[decoded_text] = f'<{char}>'\n",
        "\n",
        "    return sentinels\n",
        "\n",
        "sentinels = get_sentinels(vocab_size)\n",
        "\n",
        "\n",
        "def pretty_decode(encoded_str_list, sentinels=sentinels):\n",
        "    # If already a string, just do the replacements.\n",
        "    if isinstance(encoded_str_list, (str, bytes)):\n",
        "        for token, char in sentinels.items():\n",
        "            encoded_str_list = encoded_str_list.replace(token, char)\n",
        "        return encoded_str_list\n",
        "\n",
        "    # We need to decode and then prettyfy it.\n",
        "    return pretty_decode(detokenize(encoded_str_list))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2fbf05831d98>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       vocab_dir = path + \"/models/\"))\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m vocab_size = trax.data.vocab_size(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mvocab_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentencepiece'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentencepiece.model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/data/tf_inputs.py\u001b[0m in \u001b[0;36mvocab_size\u001b[0;34m(vocab_type, vocab_file, vocab_dir, n_reserved_ids)\u001b[0m\n\u001b[1;32m    569\u001b[0m   \"\"\"\n\u001b[1;32m    570\u001b[0m   \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_reserved_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seqio/vocabularies.py\u001b[0m in \u001b[0;36mvocab_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seqio/vocabularies.py\u001b[0m in \u001b[0;36m_base_vocab_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m       \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \"\"\"\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetPieceSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seqio/vocabularies.py\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0msentencepiece_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m\"\"\"Returns the Python tokenizer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seqio/vocabularies.py\u001b[0m in \u001b[0;36m_model_context\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     self._model = self._load_model(\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentencepiece_model_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seqio/vocabularies.py\u001b[0m in \u001b[0;36m_load_model\u001b[0;34m(cls, sentencepiece_model_file, extra_ids, normalizer_spec_overrides_serialized, reverse_extra_ids)\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;31m# Handle cases where SP can't load the file, but gfile can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentencepiece_model_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0msp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentencepiece_model_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelProto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;31m# Add placeholder strings for extra IDs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \"\"\"\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         raise errors.PermissionDeniedError(None, None,\n\u001b[1;32m     76\u001b[0m                                            \"File isn't open for reading\")\n\u001b[0;32m---> 77\u001b[0;31m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0m\u001b[1;32m     78\u001b[0m           compat.path_to_str(self.__name), 1024 * 512)\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /content/drive/My Drive/NLP_C4_W3_Colabs/models/sentencepiece.model; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEoSSKNwgDVA"
      },
      "source": [
        "<a name='3'></a>\n",
        "# Part 3: Fine-tuning on SQuAD\n",
        "\n",
        "Now let's try to fine tune on SQuAD and see what becomes of the model. For this, we need to write a function that will create and process the SQuAD `tf.data.Dataset`. Below is how T5 pre-processes SQuAD dataset as a text2text example. Before we jump in, we will have to first load in the data.\n",
        "\n",
        "<a name='3.1'></a>\n",
        "### 3.1 Loading in the data and preprocessing\n",
        "\n",
        "You first start by loading in the dataset. The text2text example for a SQuAD example looks like:\n",
        "\n",
        "```\n",
        "{\n",
        "  'inputs': 'question: <question> context: <article>',\n",
        "  'targets': '<answer_0>',\n",
        "}\n",
        "```\n",
        "\n",
        "The squad pre-processing function takes in the dataset and processes it using the sentencePiece vocabulary you have seen above. It generates the features from the vocab and encodes the string features. It takes on question, context, and answer, and returns \"question: Q context: C\" as input and \"A\" as target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcdR5Dh9UVEw"
      },
      "source": [
        "# Retrieve Question, C, A and return \"question: Q context: C\" as input and \"A\" as target.\n",
        "def squad_preprocess_fn(dataset, mode='train'):\n",
        "  return t5.data.preprocessors.squad(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NCIejwexv4x"
      },
      "source": [
        "# train generator, this takes about 1 minute\n",
        "train_generator_fn, eval_generator_fn = trax.data.tf_inputs.data_streams(\n",
        "  'squad/v1.1:3.0.0',\n",
        "  data_dir= path + '/data/',\n",
        "  bare_preprocess_fn=squad_preprocess_fn,\n",
        "  input_name='inputs',\n",
        "  target_name='targets'\n",
        ")\n",
        "train_generator = train_generator_fn()\n",
        "next(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGQsExH8xv40"
      },
      "source": [
        "#print example from train_generator\n",
        "(inp, out) = next(train_generator)\n",
        "print(inp.decode('utf8').split('context:')[0])\n",
        "print()\n",
        "print('context:', inp.decode('utf8').split('context:')[1])\n",
        "print()\n",
        "print('target:', out.decode('utf8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_eGJHpPh-rz"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 Decoding from a fine-tuned model\n",
        "\n",
        "You will now use an existing model that we trained for you. You will initialize, then load in your model, and then try with your own input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZztX5XLa01p"
      },
      "source": [
        "# Initialize the model\n",
        "model = trax.models.Transformer(\n",
        "    d_ff = 4096,\n",
        "    d_model = 1024,\n",
        "    max_len = 2048,\n",
        "    n_heads = 16,\n",
        "    dropout = 0.1,\n",
        "    input_vocab_size = 32000,\n",
        "    n_encoder_layers = 24,\n",
        "    n_decoder_layers = 24,\n",
        "    mode='predict')  # Change to 'eval' for slow decoding."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQRAGG2fnSMe"
      },
      "source": [
        "# load in the model\n",
        "# this will take a minute\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "model.init_from_file(path + '/models/model_squad.pkl.gz',\n",
        "                     weights_only=True, input_signature=(shape11, shape11))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l33umWrRyYNm"
      },
      "source": [
        "# Uncomment to see the transformer's structure.\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzrxtGGDuUkc"
      },
      "source": [
        "# create inputs\n",
        "# a simple example\n",
        "# inputs = 'question: She asked him where is john? context: John was at the game'\n",
        "\n",
        "# an extensive example\n",
        "inputs = 'question: What are some of the colours of a rose? context: A rose is a woody perennial flowering plant of the genus Rosa, in the family Rosaceae, or the flower it bears.There are over three hundred species and tens of thousands of cultivars. They form a group of plants that can be erect shrubs, climbing, or trailing, with stems that are often armed with sharp prickles. Flowers vary in size and shape and are usually large and showy, in colours ranging from white through yellows and reds. Most species are native to Asia, with smaller numbers native to Europe, North America, and northwestern Africa. Species, cultivars and hybrids are all widely grown for their beauty and often are fragrant.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVlu-iwHcZRY"
      },
      "source": [
        "# tokenizing the input so we could feed it for decoding\n",
        "print(tokenize(inputs))\n",
        "test_inputs = tokenize(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJh_Qw9G5jND"
      },
      "source": [
        "# Temperature is a parameter for sampling.\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\n",
        "                                        temperature=0.0, max_length=5)\n",
        "print(wrapper.fill(pretty_decode(output[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BadBty9C-8wY"
      },
      "source": [
        "### Note: As you can see the RAM is almost full, it is because the model and the decoding is memory heavy. You can run decoding just once. Running it the second time with another example might give you the same answer as before, or not run at all (crash). If that happens restart the runtime (see how to at the start of the notebook) and run all the cells again."
      ]
    }
  ]
}